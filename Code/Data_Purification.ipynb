{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43bbf574",
   "metadata": {},
   "source": [
    "### **Majestic Million Domains - Dataset Cleaning**\n",
    "This script is designed to process the **majestic_million.csv** dataset in order to generate a high-quality, representative subset of legitimate domain names. The resulting dataset can serve as a baseline for classification tasks or comparative analysis in supervised learning contexts.\n",
    "\n",
    "---\n",
    "\n",
    "### Objective\n",
    "To extract a curated collection of 20,000 unique, valid, and top-level domains (TLDs) that reflect legitimate web activity. This subset forms a clean, reliable benchmark for evaluating cybersquatting detection models or other domain-based security analyses later on.\n",
    "\n",
    "--- \n",
    "\n",
    "### Input and Output Details\n",
    "- **Input file:** majestic_million.csv\n",
    "- **Expected Header:** *(Insert header here)*\n",
    "- **Input path:** *(Insert full path here)*\n",
    "\n",
    "- **Output file CSV:** majestic_20k_domains.csv \n",
    "- **Output file JSONL:** majestic_20k_domains.jsonl\n",
    "- **Output path:** *(Insert full path here)* \n",
    "\n",
    "---\n",
    "\n",
    "### Processing Workflow\n",
    "0. Start the domain purification process\n",
    "1. Read the first 25.000 rows of the dataset, including the header row\n",
    "2. Retain only the first 20,000 rows for downstream processing\n",
    "3. Filter out entries where the TLD field is missing or empty\n",
    "4. Remove duplicate entries in the Domain column to ensure uniqueness\n",
    "5. Extract the full domain name from the \"Domain\" column\n",
    "6. Format the cleaned data into the following structure: id, domain, classification\n",
    "7. Label all data as \"legit\"\n",
    "8. Ensure that the output directory exists\n",
    "9. Successfully save the CSV file to the named output path\n",
    "10. Successfully save the JSONL file to the named output path\n",
    "11. Complete purification process\n",
    "\n",
    "---\n",
    "\n",
    "### Export\n",
    "- Generate and export a CSV file for general use\n",
    "- Generate and export a JSONL file for machine learning pipelines\n",
    "\n",
    "---\n",
    "\n",
    "### Logging\n",
    "Print key metrics at each processing step:\n",
    "- Number of rows read\n",
    "- Number of domains retained after filtering\n",
    "- Preview of sample entries\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237c9e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, ensure tldextract is installed. If not, run this in a separate cell:\n",
    "import sys\n",
    "!{sys.executable} -m pip install tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85511325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# File Paths & Configuration \n",
    "input_file_path = ''\n",
    "output_folder_path = ''\n",
    "output_file_name = 'majestic_20k_domains.csv'\n",
    "output_file_path = os.path.join(output_folder_path, output_file_name)\n",
    "ROWS_TO_READ = 25000\n",
    "ROWS_TO_KEEP_INITIALLY = 20000\n",
    "\n",
    "# Script \n",
    "print(\"0. Starting the domain purification process\")\n",
    "\n",
    "# Read the CSV file (first 25,000 rows)\n",
    "try:\n",
    "    df = pd.read_csv(input_file_path, nrows=ROWS_TO_READ)\n",
    "    print(f\"1. Sucessfully read {len(df)} rows including the header\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The input file was not found at {input_file_path}\")\n",
    "    print(\"Please check the file path and try again.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the CSV file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Reduction to the first 20,000 domains \n",
    "if len(df) > ROWS_TO_KEEP_INITIALLY:\n",
    "    df = df.head(ROWS_TO_KEEP_INITIALLY)\n",
    "    print(f\"2. Reduced dataset to the first {len(df)} domains\")\n",
    "else:\n",
    "    print(f\"Dataset already has {len(df)} domains or fewer, no initial reduction needed beyond the read limit\")\n",
    "\n",
    "# Only consider TLD domains \n",
    "# Assuming 'TLD' column should not be empty or NaN for valid TLDs\n",
    "initial_domain_count_before_tld_filter = len(df)\n",
    "df = df.dropna(subset=['TLD'])\n",
    "df = df[df['TLD'].astype(str).str.strip() != ''] # Ensure TLD is not just whitespace\n",
    "print(f\"\\n3. Filtering for TLD domains...\")\n",
    "print(f\"   Domains before TLD filter: {initial_domain_count_before_tld_filter}\")\n",
    "print(f\"   Domains after TLD filter: {len(df)}\")\n",
    "\n",
    "# Only consider unique domains\n",
    "# Based on the 'Domain' column\n",
    "initial_domain_count_before_uniqueness = len(df)\n",
    "df = df.drop_duplicates(subset=['Domain'], keep='first')\n",
    "print(f\"\\n4. Ensuring unique domains...\")\n",
    "print(f\"   Domains before uniqueness filter: {initial_domain_count_before_uniqueness}\")\n",
    "print(f\"   Domains after uniqueness filter: {len(df)}\")\n",
    "\n",
    "# Create the new DataFrame with specified labels \n",
    "print(f\"\\n5. Transforming data to the new format: id, domain, classification\")\n",
    "final_df = pd.DataFrame()\n",
    "final_df['domain'] = df['Domain']\n",
    "final_df['classification'] = 'legit'\n",
    "final_df.reset_index(drop=True, inplace=True) # Remove old index\n",
    "final_df.insert(0, 'id', final_df.index)      # Add new 'id' column starting from 0\n",
    "\n",
    "print(f\"\\n6. Final dataset details:\")\n",
    "print(f\"   Total legit domains prepared: {len(final_df)}\")\n",
    "if not final_df.empty:\n",
    "    print(\"   Sample data (first 3 rows):\")\n",
    "    sample_output = final_df.head(3).to_string(index=False)\n",
    "    indented_output = \"\\n\".join(\"  \" + line for line in sample_output.splitlines())\n",
    "    print(indented_output)\n",
    "else:\n",
    "    print(\"The final dataset is empty after filtering\")\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_folder_path, exist_ok=True)\n",
    "print(f\"\\n7. Ensured output directory exists: {output_folder_path}\")\n",
    "\n",
    "# Create new CSV file\n",
    "try:\n",
    "    final_df.to_csv(output_file_path, index=False)\n",
    "    print(f\"8. Successfully saved the purified data to: {output_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving the output CSV file: {e}\")\n",
    "\n",
    "# Create new JSON file\n",
    "jsonl_output_path = os.path.join(output_folder_path, 'majestic_20k_domains.jsonl')\n",
    "try:\n",
    "    with open(jsonl_output_path, 'w', encoding='utf-8') as f_jsonl:\n",
    "        for _, row in final_df.iterrows():\n",
    "            json_record = {\n",
    "                \"id\": int(row[\"id\"]),\n",
    "                \"domain\": row[\"domain\"],\n",
    "                \"classification\": row[\"classification\"]\n",
    "            }\n",
    "            f_jsonl.write(json.dumps(json_record) + \"\\n\")\n",
    "    print(f\"9. Successfully saved JSONL output to: {jsonl_output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving the output JSONL file: {e}\")\n",
    "\n",
    "\n",
    "print(\"9. Purification process completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312847d4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62b4447",
   "metadata": {},
   "source": [
    "### **DGArchive (Fraunhofer) - Dataset Cleaning**\n",
    "This script processes the **dgarchive.csv** dataset provided by the Fraunhofer Institute. The objective is to extract a representative and manageable subset of DGA-generated domains for analysis or training purposes in malicious domain detection models.\n",
    "\n",
    "---\n",
    "\n",
    "### Objective\n",
    "To curate a high-quality dataset of 20,000 unique, malicious (DGA) domain names, preserving class distribution and meaningful variation across structural and algorithmic features. This subset facilitates robust training and evaluation of detection mechanisms for generated domain names.\n",
    "\n",
    "--- \n",
    "\n",
    "### Input and Output Details\n",
    "- **Input file:** dgarchive.csv\n",
    "- **Expected Header:** *(Insert header here)*\n",
    "- **Input path:** *(Insert full path here)*\n",
    "\n",
    "- **Output file CSV:** dgarchive_20k_domains.csv \n",
    "- **Output file JSONL:** dgarchive_20k_domains.jsonl\n",
    "- **Output file PNG:** dgarchive_20k_domains_distribution.png\n",
    "- **Output path:** *(Insert full path here)* \n",
    "\n",
    "---\n",
    "\n",
    "### Processing Workflow\n",
    "0. Start the dga purification process\n",
    "1. Read the entire dataset into memory\n",
    "2. Reduce the dataset to 20,000 domains using stratified sampling based on class or DGA family labels to preserve proportional representation\n",
    "4. Ensure the sampled dataset contains varying domains based on the most represented malware families across the CSVs\n",
    "5. Extract the full domain name from the 'Domain' column\n",
    "6. Format the cleaned data into the following structure: id, domain, family, classification\n",
    "7. Label all data as \"dga\"\n",
    "8. Ensure that the output directory exists\n",
    "9. Successfully save the CSV file to the named output path\n",
    "10. Successfully save the JSONL file to the named output path\n",
    "11. Complete purification process\n",
    "\n",
    "---\n",
    "\n",
    "### Logging\n",
    "Output key metrics for verification at various steps:\n",
    "   - Total number of records\n",
    "   - Number of unique domains\n",
    "   - Class distribution before and after sampling\n",
    "   - Domain length statistics\n",
    "   - Entropy statistics\n",
    "\n",
    "---\n",
    "\n",
    "### Export\n",
    "- Generate and export a CSV file for general use\n",
    "- Generate and export a JSONL file for machine learning pipelines\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdf7439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, ensure pandas scikit-learn numpy is installed. If not, run this in a separate cell:\n",
    "import sys\n",
    "!{sys.executable} -m pip install pandas scikit-learn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9740ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# File Paths & Configuration\n",
    "input_folder = \"\"\n",
    "output_dir = \"\"\n",
    "output_csv = os.path.join(output_dir, \"dgarchive_20k_domains.csv\")\n",
    "output_jsonl = os.path.join(output_dir, \"dgarchive_20k_domains.jsonl\")\n",
    "output_png = os.path.join(output_dir, \"dgarchive_20k_domains_distribution.png\")\n",
    "\n",
    "domains_per_family = 2000\n",
    "top_n_families = 10\n",
    "\n",
    "# Script\n",
    "print(\"0. Starting the domain purification process\")\n",
    "\n",
    "# Read all datasets\n",
    "file_paths = glob(os.path.join(input_folder, \"*_dga.csv\"))\n",
    "\n",
    "family_domain_counts = {}\n",
    "all_data = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    family = os.path.basename(file_path).replace(\"_dga.csv\", \"\")\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, usecols=[0], names=[\"domain\"], skiprows=1)\n",
    "        df[\"malware_family\"] = family\n",
    "        family_domain_counts[family] = len(df)\n",
    "        all_data.append(df)\n",
    "        print(f\"   Loaded {len(df)} domains from family: {family}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Error processing {file_path}: {e}\")\n",
    "print(\"1. Read the entire dataset into memory\")\n",
    "\n",
    "\n",
    "# Combine all CSVs\n",
    "df_all = pd.concat(all_data, ignore_index=True)\n",
    "print(\"2. Combined all datasets to one dataset\")\n",
    "\n",
    "# Select top-N most represented families\n",
    "df_counts = pd.DataFrame.from_dict(family_domain_counts, orient='index', columns=['count'])\n",
    "df_counts = df_counts.sort_values(by='count', ascending=False)\n",
    "selected_families = df_counts.head(top_n_families).index.tolist()\n",
    "df_top = df_all[df_all[\"malware_family\"].isin(selected_families)]\n",
    "print(\"3. Selected top-N most represented families\")\n",
    "\n",
    "\n",
    "# Establish new CSV structure\n",
    "df_sampled = (\n",
    "    df_top.groupby(\"malware_family\")\n",
    "    .apply(lambda x: x.sample(n=domains_per_family, random_state=42))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df_sampled = df_sampled.rename(columns={\"malware_family\": \"family\"})\n",
    "print(\"4. Established new CSV structure: id, domain, family, classification\")\n",
    "\n",
    "# Data labeling\n",
    "df_sampled[\"classification\"] = \"dga\"\n",
    "df_sampled.insert(0, \"id\", range(len(df_sampled)))\n",
    "print(\"5. Classified all data as 'dga'\")\n",
    "\n",
    "# Ensure that the output directory exists\n",
    "def ensure_output_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        print(f\"   Created output directory: {path}\")\n",
    "    else:\n",
    "        print(f\"   Output directory already exists: {path}\")\n",
    "ensure_output_dir(output_dir)\n",
    "print(\"6. Ensured that the output directory exists\")\n",
    "\n",
    "# Save PNG\n",
    "family_summary = df_sampled['family'].value_counts()\n",
    "plt.figure(figsize=(10, 6))\n",
    "family_summary.plot(kind=\"bar\", color='#59c3b2', edgecolor=\"black\")\n",
    "plt.title(\"Final selected malware families (20,000 DGA domains)\")\n",
    "plt.ylabel(\"Number of domains\")\n",
    "plt.xlabel(\"Malware family\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_png)\n",
    "plt.close()\n",
    "print(f\"\\n7. Successfully saved PNG output to: {output_png}\")\n",
    "\n",
    "# Save CSV\n",
    "df_sampled.to_csv(output_csv, index=False)\n",
    "print(f\"8. Successfully saved CSV output to: {output_csv}\")\n",
    "\n",
    "# --- NEU: Save distribution CSV ---\n",
    "distribution_csv = os.path.join(output_dir, \"dgarchive_20k_domains_distribution.csv\")\n",
    "family_summary_df = family_summary.rename_axis('family').reset_index(name='count')\n",
    "family_summary_df.to_csv(distribution_csv, index=False)\n",
    "print(f\"8b. Successfully saved distribution CSV to: {distribution_csv}\")\n",
    "\n",
    "# Save JSONL\n",
    "with open(output_jsonl, \"w\") as f:\n",
    "    for _, row in df_sampled.iterrows():\n",
    "        json.dump(row.to_dict(), f)\n",
    "        f.write(\"\\n\")\n",
    "print(f\"9. Successfully saved JSONL output to: {output_jsonl}\")\n",
    "\n",
    "# Final step\n",
    "print(\"10. Purification process complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ec920d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693ca633",
   "metadata": {},
   "source": [
    "### **Cybersquatting Variant Generation — Balanced Dataset**\n",
    "\n",
    "This script systematically generates a diverse set of cybersquatting domain variants based on the **majestic_20k_domains.csv** dataset. The objective is to construct a balanced and high-quality dataset for research in phishing detection, domain abuse analysis, and machine learning training related to cybersquatting threats.\n",
    "\n",
    "---\n",
    "\n",
    "### Objective\n",
    "\n",
    "To generate 20,000 unique, synthetically created cybersquatting domains using multiple transformation techniques applied to legitimate domains. Each entry is labeled with its generation strategy to support supervised classification and adversarial robustness evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "### Input and Output Details\n",
    "- **Input file**: majestic_20k_domains.csv  \n",
    "- **Expected Header:** *(Insert header here)*\n",
    "- **Input path:** *(Insert full path here)*\n",
    "\n",
    "- **Output file CSV:** squatting_20k_domains_balanced.csv\n",
    "- **Output file CSV:** squatting_20k_domains_balanced_distribution.csv\n",
    "- **Output file JSONL:** squatting_20k_domains_unbalanced_distribution.jsonl\n",
    "- **Output file PNG:** squatting_20k_domains_balanced_distribution.png\n",
    "- **Output path:** *(Insert full path here)* \n",
    "\n",
    "---\n",
    "\n",
    "### Processing Workflow\n",
    "1. Import the list of legitimate domains from the CSV file\n",
    "2. Remove any invalid, empty, or malformed domain entries\n",
    "\n",
    "3. For each target domain, apply a range of transformation strategies to generate cybersquatting variants:\n",
    "   1. Leetspeak substitutions: e.g., google.com → g00gle.com\n",
    "   2. Typographic errors / letter swaps: e.g., paypal.com → payapl.com\n",
    "   3. Missing char: e.g., netflix.com → netflx.com\n",
    "   4. Extra char: e.g., google.com → gooogle.com\n",
    "   4. Homoglyph replacements: e.g., using characters from other scripts like Cyrillic\n",
    "   5. TLD swap: e.g., spotify.com → spotify.net\n",
    "   6. Subdomain attack: e.g., secure.login.paypal.com\n",
    "   7. Combo squatting: e.g., getpatreon.com, login-facebook.com\n",
    "   8. Bitsquatting: e.g., based on single-bit errors in characters\n",
    "   9. Keyword insertion: e.g., secure-amazon.com, update-ebay.com\n",
    "\n",
    "4. Ensure that each squatting method contributes evenly to the final dataset\n",
    "5. Avoid duplicates by tracking previously generated variants\n",
    "6. Format the cleaned data into the following structure: id, target_domain, domain, classification, squat_type\n",
    "7. Classify all data as \"squat\"\n",
    "8. Ensure that the output directory exists\n",
    "9. Successfully save the CSV file to the named output path\n",
    "10. Successfully save the JSONL file to the named output path\n",
    "11. Complete purification process\n",
    "\n",
    "---\n",
    "\n",
    "### Logging\n",
    "Output key metrics for verification at various steps:\n",
    "   - Total number of generated domains\n",
    "   - Squatting method distribution (percentages)\n",
    "\n",
    "---\n",
    "\n",
    "### Export\n",
    "- Generate and export a CSV file for general use\n",
    "- Generate and export a CSV file for distribution overview\n",
    "- Generate and export a JSONL file for machine learning pipelines\n",
    "- Generate a PNG file for overview\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1fcd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, ensure pandas scikit-learn numpy is installed. If not, run this in a separate cell:\n",
    "import sys\n",
    "!{sys.executable} -m pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f65cc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#balanced dataset\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# File Paths & Configuration \n",
    "input_file = \"\"\n",
    "output_dir = \"\"\n",
    "output_csv = os.path.join(output_dir, \"squatting_20k_domains_balanced.csv\")\n",
    "dist_csv = os.path.join(output_dir, \"squatting_20k_domains_balanced_distribution.csv\")\n",
    "dist_png = os.path.join(output_dir, \"squatting_20k_domains_balanced_distribution.png\")\n",
    "\n",
    "# Script \n",
    "print(\"0. Starting the cybersquatting generation process (balanced)\")\n",
    "\n",
    "# Mapping tables\n",
    "leetspeak_map = {'a': ['4', '@'], 'e': ['3'], 'i': ['1', '!'], 'o': ['0'], 's': ['5', '$'], 'g': ['9'], 'l': ['1']}\n",
    "homoglyph_map = {'a': ['а'], 'e': ['е'], 'i': ['і', 'í'], 'o': ['ο', 'о'], 'c': ['с'], 'd': ['ԁ'], 'p': ['р'], 'y': ['у'], 'h': ['һ'], 'm': ['rn']}\n",
    "keywords = ['login', 'secure', 'verify', 'signin', 'update', 'support']\n",
    "combos = ['bankof', 'my', 'get', 'try']\n",
    "tlds = ['net', 'xyz', 'shop', 'info', 'online', 'click']\n",
    "subdomains = ['login', 'secure', 'auth', 'mail', 'verify', 'account']\n",
    "\n",
    "# Transformation functions\n",
    "def apply_leetspeak(domain):\n",
    "    return ''.join(random.choice(leetspeak_map.get(c.lower(), [c])) if c.lower() in leetspeak_map and random.random() < 0.4 else c for c in domain), \"leetspeak\"\n",
    "\n",
    "def change_tld(domain):\n",
    "    base = domain.split('.')[0]\n",
    "    return base + '.' + random.choice(tlds), \"tld_swap\"\n",
    "\n",
    "def insert_keyword(domain):\n",
    "    base, tld = domain.rsplit('.', 1)\n",
    "    return base + '-' + random.choice(keywords) + '.' + tld, \"keyword_insertion\"\n",
    "\n",
    "def typo_swap(domain):\n",
    "    base, tld = domain.split('.')[0], '.' + domain.split('.')[-1]\n",
    "    if len(base) < 2: return domain, \"typo_swap\"\n",
    "    i = random.randint(0, len(base) - 2)\n",
    "    swapped = list(base)\n",
    "    swapped[i], swapped[i+1] = swapped[i+1], swapped[i]\n",
    "    return ''.join(swapped) + tld, \"typo_swap\"\n",
    "\n",
    "def missing_char(domain):\n",
    "    base, tld = domain.split('.')[0], '.' + domain.split('.')[-1]\n",
    "    if len(base) < 2: return domain, \"missing_char\"\n",
    "    i = random.randint(0, len(base) - 1)\n",
    "    return base[:i] + base[i+1:] + tld, \"missing_char\"\n",
    "\n",
    "def extra_char(domain):\n",
    "    base, tld = domain.split('.')[0], '.' + domain.split('.')[-1]\n",
    "    i = random.randint(0, len(base))\n",
    "    return base[:i] + random.choice('abcdefghijklmnopqrstuvwxyz') + base[i:] + tld, \"extra_char\"\n",
    "\n",
    "def subdomain_attack(domain):\n",
    "    base = domain.split('.')[0]\n",
    "    return base + \".\" + random.choice(subdomains) + \"-update.com\", \"subdomain_attack\"\n",
    "\n",
    "def combo_squatting(domain):\n",
    "    base, tld = domain.split('.')[0], '.' + domain.split('.')[-1]\n",
    "    return random.choice(combos) + base + tld, \"combo_squatting\"\n",
    "\n",
    "def apply_homoglyph(domain):\n",
    "    base, tld = domain.split('.')[0], '.' + domain.split('.')[-1]\n",
    "    result = ''.join(random.choice(homoglyph_map[c.lower()]) if c.lower() in homoglyph_map and random.random() < 0.4 else c for c in base)\n",
    "    return result + tld, \"homoglyph\"\n",
    "\n",
    "def apply_bitsquatting(domain):\n",
    "    base, tld = domain.split('.')[0], '.' + domain.split('.')[-1]\n",
    "    if not base: return domain, \"bitsquatting\"\n",
    "    i = random.randint(0, len(base) - 1)\n",
    "    bitflip = chr(ord(base[i]) ^ 1)\n",
    "    return base[:i] + bitflip + base[i+1:] + tld, \"bitsquatting\"\n",
    "\n",
    "methods = [\n",
    "    apply_leetspeak,\n",
    "    change_tld,\n",
    "    insert_keyword,\n",
    "    typo_swap,\n",
    "    missing_char,\n",
    "    extra_char,\n",
    "    subdomain_attack,\n",
    "    combo_squatting,\n",
    "    apply_homoglyph,\n",
    "    apply_bitsquatting\n",
    "]\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(input_file)\n",
    "print(f\"1. Sucessfully read {len(df)} rows including the header\")\n",
    "domains = df[\"domain\"].dropna().unique().tolist()\n",
    "generated, results = set(), []\n",
    "\n",
    "samples_per_method = 2000\n",
    "for func in methods:\n",
    "    count = 0\n",
    "    while count < samples_per_method:\n",
    "        target = random.choice(domains)\n",
    "        fake, mtype = func(target)\n",
    "        if fake not in generated:\n",
    "            generated.add(fake)\n",
    "            results.append((len(results), target, fake, \"squat\", mtype))\n",
    "            count += 1\n",
    "\n",
    "print(\"\\n2. Generating 20k balanced samples\")\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"\\n3. Ensured output directory exists: {output_dir}\")\n",
    "\n",
    "# Export CSV\n",
    "df_out = pd.DataFrame(results, columns=[\"id\", \"target_domain\", \"domain\", \"classification\", \"squat_type\"])\n",
    "df_out.to_csv(output_csv, index=False)\n",
    "print(f\"4. Successfully saved CSV output to: {output_csv}\")\n",
    "df_out['squat_type'].value_counts().to_csv(dist_csv)\n",
    "\n",
    "# Export JSONL \n",
    "jsonl_path = os.path.join(output_dir, \"squatting_20k_domains_balanced.jsonl\")\n",
    "with open(jsonl_path, 'w', encoding='utf-8') as f_jsonl:\n",
    "    for _, row in df_out.iterrows():\n",
    "        json_record = {\n",
    "            \"id\": int(row[\"id\"]),\n",
    "            \"target_domain\": row[\"target_domain\"],\n",
    "            \"domain\": row[\"domain\"],\n",
    "            \"classification\": row[\"classification\"],\n",
    "            \"squat_type\": row[\"squat_type\"]\n",
    "        }\n",
    "        f_jsonl.write(json.dumps(json_record) + \"\\n\")\n",
    "print(f\"5. Successfully saved the JSONL output to: {jsonl_path}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "df_out['squat_type'].value_counts().plot(kind=\"bar\", color='#59c3b2', edgecolor=\"black\")\n",
    "plt.title(\"Distribution of Cybersquatting Types (Balanced, 20k)\")\n",
    "plt.ylabel(\"Number of Domains\")\n",
    "plt.xlabel(\"Cybersquatting Types\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(dist_png)\n",
    "print(\"\\n6. Data plot generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91869e8a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d94ca9",
   "metadata": {},
   "source": [
    "### **Cybersquatting Variant Generation — Realistic / Unbalanced Dataset**\n",
    "\n",
    "This script generates a diverse set of cybersquatting domain variants from the **majestic_20k_domains.csv** dataset. Unlike the balanced version, this script emphasizes a realistic distribution of squatting techniques based on observations from phishing campaigns and domain abuse incidents.\n",
    "\n",
    "---\n",
    "\n",
    "### Objective\n",
    "\n",
    "To synthesize 20,000 cybersquatting domains using a distribution that mimics real-world prevalence of attack methods. The dataset aims to reflect asymmetric technique usage patterns and is suited for training or evaluating detection systems in realistic conditions.\n",
    "\n",
    "---\n",
    "\n",
    "### Input and Output Details\n",
    "- **Input file**: majestic_20k_domains.csv  \n",
    "- **Expected Header:** *(Insert header here)*\n",
    "- **Input path:** *(Insert full path here)*\n",
    "\n",
    "- **Output file CSV:** squatting_20k_domains_unbalanced.csv\n",
    "- **Output file CSV:** squatting_20k_domains_unbalanced_distribution.csv\n",
    "- **Output file JSONL:** squatting_20k_domains_unbalanced_distribution.jsonl\n",
    "- **Output file PNG:** squatting_20k_domains_unbalanced_distribution.png\n",
    "- **Output path:** *(Insert full path here)* \n",
    "\n",
    "---\n",
    "\n",
    "### Processing Workflow\n",
    "1. Import the list of legitimate domains from the CSV file\n",
    "2. Remove any invalid, empty, or malformed domain entries\n",
    "\n",
    "3. For each target domain, apply a range of transformation strategies to generate cybersquatting variants:\n",
    "   1. Leetspeak substitutions (10%): e.g., google.com → g00gle.com\n",
    "   2. Typographic errors / letter swaps (30%): e.g., paypal.com → payapl.com\n",
    "   3. Missing char (6%): e.g., netflix.com → netflx.com\n",
    "   4. Extra char (5%): e.g., google.com → gooogle.com\n",
    "   4. Homoglyph replacements (6%): e.g., using characters from other scripts like Cyrillic\n",
    "   5. TLD swap (10%): e.g., spotify.com → spotify.net\n",
    "   6. Subdomain attack (8%): e.g., secure.login.paypal.com\n",
    "   7. Combo squatting (10%): e.g., getpatreon.com, login-facebook.com\n",
    "   8. Bitsquatting (4%): e.g., based on single-bit errors in characters\n",
    "   9. Keyword insertion (10%): e.g., secure-amazon.com, update-ebay.com\n",
    "\n",
    "4. Ensure that each squatting method contributes evenly (stratified sampling) to the final dataset\n",
    "5. Avoid duplicates by tracking previously generated variants\n",
    "6. Format the cleaned data into the following structure: id, target_domain, domain, classification, squat_type\n",
    "7. Classify all data as \"squat\"\n",
    "8. Ensure that the output directory exists\n",
    "9. Successfully save the CSV file to the named output path\n",
    "10. Successfully save the JSONL file to the named output path\n",
    "11. Complete purification process\n",
    "\n",
    "---\n",
    "\n",
    "### Logging\n",
    "Output key metrics for verification at various steps:\n",
    "   - Total number of generated domains\n",
    "   - Squatting method distribution (percentages)\n",
    "\n",
    "---\n",
    "\n",
    "### Export\n",
    "- Generate and export a CSV file for general use\n",
    "- Generate and export a CSV file for distribution overview\n",
    "- Generate and export a JSONL file for machine learning pipelines\n",
    "- Generate a PNG file for overview\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008b1ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unbalanced dataset\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# File Paths & Confugration\n",
    "input_file = \"\"\n",
    "output_dir = \"\"\n",
    "output_csv = os.path.join(output_dir, \"squatting_20k_domains_unbalanced.csv\")\n",
    "dist_csv = os.path.join(output_dir, \"squatting_20k_domains_unbalanced_distribution.csv\")\n",
    "dist_png = os.path.join(output_dir, \"squatting_20k_domains_unbalanced_distribution.png\")\n",
    "\n",
    "# Script\n",
    "print(\"0. Starting the cybersquatting generation process (unbalanced)\")\n",
    "\n",
    "# Mapping Tables\n",
    "leetspeak_map = {'a': ['4', '@'], 'e': ['3'], 'i': ['1', '!'], 'o': ['0'], 's': ['5', '$'], 'g': ['9'], 'l': ['1']}\n",
    "homoglyph_map = {'a': ['а'], 'e': ['е'], 'i': ['і', 'í'], 'o': ['ο', 'о'], 'c': ['с'], 'd': ['ԁ'], 'p': ['р'], 'y': ['у'], 'h': ['һ'], 'm': ['rn']}\n",
    "keywords = ['login', 'secure', 'verify', 'signin', 'update', 'support']\n",
    "combos = ['bankof', 'my', 'get', 'try']\n",
    "tlds = ['net', 'xyz', 'shop', 'info', 'online', 'click']\n",
    "subdomains = ['login', 'secure', 'auth', 'mail', 'verify', 'account']\n",
    "\n",
    "# Transformation functions (same as in balanced version)\n",
    "def apply_leetspeak(domain):\n",
    "    return ''.join(random.choice(leetspeak_map.get(c.lower(), [c])) if c.lower() in leetspeak_map and random.random() < 0.4 else c for c in domain), \"leetspeak\"\n",
    "\n",
    "def change_tld(domain):\n",
    "    base = domain.split('.')[0]\n",
    "    return base + '.' + random.choice(tlds), \"tld_swap\"\n",
    "\n",
    "def insert_keyword(domain):\n",
    "    base, tld = domain.rsplit('.', 1)\n",
    "    return base + '-' + random.choice(keywords) + '.' + tld, \"keyword_insertion\"\n",
    "\n",
    "def typo_swap(domain):\n",
    "    base, tld = domain.split('.')[0], '.' + domain.split('.')[-1]\n",
    "    if len(base) < 2: return domain, \"typo_swap\"\n",
    "    i = random.randint(0, len(base) - 2)\n",
    "    swapped = list(base)\n",
    "    swapped[i], swapped[i+1] = swapped[i+1], swapped[i]\n",
    "    return ''.join(swapped) + tld, \"typo_swap\"\n",
    "\n",
    "def missing_char(domain):\n",
    "    base, tld = domain.split('.')[0], '.' + domain.split('.')[-1]\n",
    "    if len(base) < 2: return domain, \"missing_char\"\n",
    "    i = random.randint(0, len(base) - 1)\n",
    "    return base[:i] + base[i+1:] + tld, \"missing_char\"\n",
    "\n",
    "def extra_char(domain):\n",
    "    base, tld = domain.split('.')[0], '.' + domain.split('.')[-1]\n",
    "    i = random.randint(0, len(base))\n",
    "    return base[:i] + random.choice('abcdefghijklmnopqrstuvwxyz') + base[i:] + tld, \"extra_char\"\n",
    "\n",
    "def subdomain_attack(domain):\n",
    "    base = domain.split('.')[0]\n",
    "    return base + \".\" + random.choice(subdomains) + \"-update.com\", \"subdomain_attack\"\n",
    "\n",
    "def combo_squatting(domain):\n",
    "    base, tld = domain.split('.')[0], '.' + domain.split('.')[-1]\n",
    "    return random.choice(combos) + base + tld, \"combo_squatting\"\n",
    "\n",
    "def apply_homoglyph(domain):\n",
    "    base, tld = domain.split('.')[0], '.' + domain.split('.')[-1]\n",
    "    result = ''.join(random.choice(homoglyph_map[c.lower()]) if c.lower() in homoglyph_map and random.random() < 0.4 else c for c in base)\n",
    "    return result + tld, \"homoglyph\"\n",
    "\n",
    "def apply_bitsquatting(domain):\n",
    "    base, tld = domain.split('.')[0], '.' + domain.split('.')[-1]\n",
    "    if not base: return domain, \"bitsquatting\"\n",
    "    i = random.randint(0, len(base) - 1)\n",
    "    bitflip = chr(ord(base[i]) ^ 1)\n",
    "    return base[:i] + bitflip + base[i+1:] + tld, \"bitsquatting\"\n",
    "\n",
    "# Weighted list\n",
    "methods = [\n",
    "    (apply_leetspeak, 0.10),\n",
    "    (change_tld, 0.10),\n",
    "    (insert_keyword, 0.10),\n",
    "    (typo_swap, 0.30),\n",
    "    (missing_char, 0.06),\n",
    "    (extra_char, 0.05),\n",
    "    (subdomain_attack, 0.08),\n",
    "    (combo_squatting, 0.10),\n",
    "    (apply_homoglyph, 0.06),\n",
    "    (apply_bitsquatting, 0.04),\n",
    "]\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(input_file)\n",
    "print(f\"1. Sucessfully read {len(df)} rows including the header\")\n",
    "domains = df[\"domain\"].dropna().unique().tolist()\n",
    "generated, results = set(), []\n",
    "method_counter = Counter()\n",
    "\n",
    "while len(generated) < 20_000:\n",
    "    funcs, weights = zip(*methods)\n",
    "    func = random.choices(funcs, weights=weights, k=1)[0]\n",
    "    target = random.choice(domains)\n",
    "    fake, mtype = func(target)\n",
    "    if fake not in generated:\n",
    "        generated.add(fake)\n",
    "        results.append((len(results), target, fake, \"squat\", mtype))\n",
    "        method_counter[mtype] += 1\n",
    "\n",
    "print(\"\\n2. Generating 20k unbalanced samples\")\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_folder_path, exist_ok=True)\n",
    "print(f\"\\n3. Ensured output directory exists: {output_folder_path}\")\n",
    "\n",
    "# Exort CSV\n",
    "df_out = pd.DataFrame(results, columns=[\"id\", \"target_domain\", \"domain\", \"classification\", \"squat_type\"])\n",
    "df_out.to_csv(output_csv, index=False)\n",
    "print(f\"4. Successfully saved the CSV output to: {output_file_path}\")\n",
    "pd.DataFrame.from_dict(method_counter, orient='index', columns=['count']).to_csv(dist_csv)\n",
    "\n",
    "# Export JSONL \n",
    "jsonl_path = os.path.join(output_dir, \"squatting_20k_domains_unbalanced.jsonl\")\n",
    "with open(jsonl_path, 'w', encoding='utf-8') as f_jsonl:\n",
    "    for _, row in df_out.iterrows():\n",
    "        json_record = {\n",
    "            \"id\": int(row[\"id\"]),\n",
    "            \"target_domain\": row[\"target_domain\"],\n",
    "            \"domain\": row[\"domain\"],\n",
    "            \"classification\": row[\"classification\"],\n",
    "            \"squat_type\": row[\"squat_type\"]\n",
    "        }\n",
    "        f_jsonl.write(json.dumps(json_record) + \"\\n\")\n",
    "print(f\"5. Successfully saved the JSONL output to: {jsonl_path}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "pd.Series(method_counter).sort_values(ascending=False).plot(kind=\"bar\", color='#59c3b2', edgecolor=\"black\")\n",
    "plt.title(\"Distribution of Cybersquatting Types (Unbalanced, 20k)\")\n",
    "plt.ylabel(\"Number of Domains\")\n",
    "plt.xlabel(\"Cybersquatting Types\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(dist_png)\n",
    "print(\"\\n6. Data plot generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09fce0f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7acc0e2",
   "metadata": {},
   "source": [
    "### **Dataset Fusion for LLM Finetuning**\n",
    "\n",
    "This script merges multiple domain datasets—covering **legitimate**, **DGA-generated**, and **cybersquatted** domains—into a single, balanced dataset. The resulting dataset is optimized for fine-tuning large language models (LLMs) in the context of domain classification and threat detection.\n",
    "\n",
    "---\n",
    "\n",
    "### Objective\n",
    "\n",
    "To generate a balanced and diverse dataset comprising 33,000 domain entries (10,000 per class: **legit**, **DGA**, **cybersquatting**) suitable for LLM fine-tuning. The final dataset is stratified and split into training, validation, and test sets (80/10/10), ensuring class balance across all subsets.\n",
    "\n",
    "---\n",
    "\n",
    "### Input and Output Details\n",
    "**File 1**\n",
    "- **Input file**: majestic_20k_domains.csv  \n",
    "- **Expected Header:** id,domain,classification\n",
    "- **Input path:** \n",
    "\n",
    "**File 2**\n",
    "- **Input file**: squatting_20k_domains_balanced.csv\n",
    "- **Expected Header:** id,target_domain,domain,classification,squat_type\n",
    "- **Input path:** \n",
    "\n",
    "**File 3**\n",
    "- **Input file**: synethized_dga_domains.csv\n",
    "- **Expected Header:** domain, family\n",
    "- **Input path:** \n",
    "\n",
    "**Output**\n",
    "- **Output file CSV:** dga_squatting_legit_data_balanced.csv\n",
    "- **Output file JSONL:** dga_squatting_legit_data_balanced.jsonl\n",
    "- **Output file CSV:** dga_squatting_legit_data_balanced_distribution.csv\n",
    "- **Output file JSONL:** dga_squatting_legit_data_balanced_distribution.jsonl\n",
    "- **Output file PNG:** dga_squatting_legit_data_balanced.png\n",
    "- **Output path:** \n",
    "\n",
    "---\n",
    "\n",
    "### Processing Workflow\n",
    "1. Load all input CSV files\n",
    "2. Standardize column names and structure across datasets\n",
    "3. Clean data:\n",
    "   - Remove invalid, empty, or malformed domain entries\n",
    "   - Remove duplicate entries across and within datasets\n",
    "4. Normalize and relabel domain classes:\n",
    "   - Legitimate (legit) = 0\n",
    "   - DGA (dga) = 1\n",
    "   - Cybersquatting (squat) = 2\n",
    "5. Stratify and sample:\n",
    "   - Select 10,000 domains per class, resulting in 30,000 total entries\n",
    "   - Assign a uniform label schema and ensure consistency\n",
    "6. Split dataset:\n",
    "   - 80% for training (split = train)\n",
    "   - 10% validation (split = val)\n",
    "   - 10% test (split = test)\n",
    "   - Maintain class balance across splits.\n",
    "7. Format and export:\n",
    "   - Add columns: id,domain,classification,split\n",
    "8. Ensure output directory exists\n",
    "9. Save all output files in CSV, JSONL, and PNG formats\n",
    "\n",
    "---\n",
    "\n",
    "### Logging\n",
    "Output key metrics for verification at various steps:\n",
    "    - Total number of input and final domain entries\n",
    "    - Number of duplicates removed\n",
    "    - Final distribution of each domain class\n",
    "    - Distribution across splits (train, val, test)\n",
    "\n",
    "---\n",
    "\n",
    "### Export\n",
    "- Generate and export a CSV file for general use\n",
    "- Generate and export a CSV file for distribution overview\n",
    "- Generate and export a JSONL file for machine learning pipelines\n",
    "- Generate a PNG file for overview\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28772928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c737b857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "# 1. Define input file paths\n",
    "input_paths = {\n",
    "    \"legit\": \"\",\n",
    "    \"squat\": \"\",\n",
    "    \"dga\": \"\"\n",
    "}\n",
    "\n",
    "# 2. Output directory and filenames\n",
    "output_dir = \"\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_files = {\n",
    "    \"csv\": os.path.join(output_dir, \"dga_squatting_legit_data_balanced.csv\"),\n",
    "    \"jsonl\": os.path.join(output_dir, \"dga_squatting_legit_data_balanced.jsonl\"),\n",
    "    \"csv_dist\": os.path.join(output_dir, \"dga_squatting_legit_data_balanced_distribution.csv\"),\n",
    "    \"jsonl_dist\": os.path.join(output_dir, \"dga_squatting_legit_data_balanced_distribution.jsonl\"),\n",
    "    \"png\": os.path.join(output_dir, \"dga_squatting_legit_data_balanced.png\")\n",
    "}\n",
    "\n",
    "# 3. Load and clean datasets\n",
    "def load_and_clean():\n",
    "    df_legit = pd.read_csv(input_paths[\"legit\"])[['domain']].dropna().drop_duplicates()\n",
    "    df_squat = pd.read_csv(input_paths[\"squat\"])[['domain']].dropna().drop_duplicates()\n",
    "    df_dga   = pd.read_csv(input_paths[\"dga\"])[['domain']].dropna().drop_duplicates()\n",
    "\n",
    "    df_legit['classification'] = 0\n",
    "    df_dga['classification'] = 1\n",
    "    df_squat['classification'] = 2\n",
    "\n",
    "    return df_legit, df_squat, df_dga\n",
    "\n",
    "# 4. Sample and combine\n",
    "def sample_and_combine(df_legit, df_squat, df_dga, n=10000):\n",
    "    df_legit = df_legit.sample(n=n, random_state=42)\n",
    "    df_squat = df_squat.sample(n=n, random_state=42)\n",
    "    df_dga   = df_dga.sample(n=n, random_state=42)\n",
    "    df_combined = pd.concat([df_legit, df_squat, df_dga], ignore_index=True)\n",
    "    return df_combined\n",
    "\n",
    "# 5. Stratified split\n",
    "# 5. Stratified split\n",
    "def stratified_split(df):\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    split_dfs = []\n",
    "\n",
    "    for class_label in sorted(df['classification'].unique()):\n",
    "        df_class = df[df['classification'] == class_label].reset_index(drop=True)\n",
    "        n = len(df_class)\n",
    "        train_end = int(0.8 * n)\n",
    "        val_end = int(0.9 * n)\n",
    "\n",
    "        df_class.loc[:train_end - 1, 'split'] = 'train'\n",
    "        df_class.loc[train_end:val_end - 1, 'split'] = 'val'\n",
    "        df_class.loc[val_end:, 'split'] = 'test'\n",
    "\n",
    "        split_dfs.append(df_class)\n",
    "\n",
    "    df_split = pd.concat(split_dfs).reset_index(drop=True)\n",
    "    df_split['id'] = range(len(df_split))  # ID von 0 bis 29999\n",
    "    return df_split\n",
    "\n",
    "# 6. Save files\n",
    "def save_outputs(df):\n",
    "    df = df[['id', 'domain', 'classification', 'split']]  # enforce column order\n",
    "\n",
    "    # Save CSV\n",
    "    df.to_csv(output_files[\"csv\"], index=False)\n",
    "\n",
    "    # Save JSONL\n",
    "    with open(output_files[\"jsonl\"], 'w') as f_jsonl:\n",
    "        for _, row in df.iterrows():\n",
    "            f_jsonl.write(json.dumps(row.to_dict()) + '\\n')\n",
    "\n",
    "    # Save distribution summary\n",
    "    dist = df.groupby(['classification', 'split']).size().reset_index(name='count')\n",
    "    dist.to_csv(output_files[\"csv_dist\"], index=False)\n",
    "    with open(output_files[\"jsonl_dist\"], 'w') as f_jsonld:\n",
    "        for _, row in dist.iterrows():\n",
    "            f_jsonld.write(json.dumps(row.to_dict()) + '\\n')\n",
    "\n",
    "    # Save plot\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.countplot(data=df, x='split', hue='classification')\n",
    "    plt.title('Class Distribution by Split')\n",
    "    plt.savefig(output_files[\"png\"])\n",
    "    plt.close()\n",
    "\n",
    "# 7. Log info\n",
    "def log_info(df):\n",
    "    label_map = {0: 'legit', 1: 'dga', 2: 'squat'}\n",
    "    \n",
    "    print(\"1. Total number of domain entries:\", len(df))\n",
    "    \n",
    "    # Class distribution\n",
    "    print(\"\\n2. Class distribution (0 = legit, 1 = dga, 2 = squat):\")\n",
    "    class_dist = df['classification'].value_counts().sort_index()\n",
    "    class_df = pd.DataFrame({\n",
    "        'classification': class_dist.index,\n",
    "        'label': [label_map[c] for c in class_dist.index],\n",
    "        'count': class_dist.values\n",
    "    })\n",
    "    print(class_df[['classification', 'count']].to_string(index=False))\n",
    "    \n",
    "    # Split distribution\n",
    "    print(\"\\n3. Distribution across splits:\")\n",
    "    split_dist = df.groupby(['classification', 'split']).size().unstack(fill_value=0).sort_index()\n",
    "    print(split_dist)\n",
    "    \n",
    "    # Cumulative totals per split\n",
    "    print(\"\\n4. Cumulative totals per split (legit + dga + squat):\")\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        split_total = split_dist[split].sum()\n",
    "        print(f\"   {split}: {split_total} entries\")\n",
    "        \n",
    "    # Sanity check\n",
    "    total_by_splits = split_dist.sum().sum()\n",
    "    total_entries = len(df)\n",
    "    print(f\"\\n5. Sanity Check: {total_by_splits} total entries across splits (should be {total_entries})\")\n",
    "    assert total_by_splits == total_entries, \"Sum of splits does not match total entries!\"\n",
    "\n",
    "# 8. Execute all\n",
    "df_legit, df_squat, df_dga = load_and_clean()\n",
    "df_combined = sample_and_combine(df_legit, df_squat, df_dga)\n",
    "df_split = stratified_split(df_combined)\n",
    "save_outputs(df_split)\n",
    "log_info(df_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1b85e8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0bf896",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in\n",
    "df = pd.read_csv(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e492c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['split'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6032e84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.crosstab(df['split'], df['classification']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec874217",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['classification'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65d54e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Pfad zur CSV-Datei (anpassen!)\n",
    "csv_path = \"\"\n",
    "\n",
    "# CSV einlesen\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Prüfen, ob die Spalte \"domain\" vorhanden ist\n",
    "if 'domain' not in df.columns:\n",
    "    raise ValueError(\"Die Spalte 'domain' wurde nicht in der CSV gefunden.\")\n",
    "\n",
    "# Längste Domain berechnen\n",
    "df['domain_length'] = df['domain'].astype(str).apply(len)\n",
    "max_length = df['domain_length'].max()\n",
    "longest_domains = df[df['domain_length'] == max_length]['domain'].tolist()\n",
    "\n",
    "# Ergebnisse ausgeben\n",
    "print(f\"Längste Domain-Länge: {max_length} Zeichen\")\n",
    "print(\"Domain(s) mit dieser Länge:\")\n",
    "for domain in longest_domains:\n",
    "    print(domain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
